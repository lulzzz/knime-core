<?xml version="1.0" encoding="utf-8"?>
<knimeNode icon="./logistic_regression_learn.png" type="Learner"
    xmlns="http://knime.org/node/v2.10" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://knime.org/node/v2.10 http://knime.org/node/v2.10.xsd">
    <name>Logistic Regression Learner</name>
    <shortDescription>Performs a multinomial logistic regression.</shortDescription>
    <fullDescription>
        <intro>
            Performs a multinomial logistic regression. Select in the dialog a 
            target column (combo box on top), i.e. the response.
            The solver combo box allows you to select which solver should be used for the problem
            (see below for details on the different solvers).
            The two
            lists in the center of the dialog allow you to include only certain 
            columns which represent the (independent) variables.
            Make sure the columns you want to have included being in the right
            "include" list.
            
            See article in wikipedia about 
            <a href="http://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>
            for an overview about the topic.
            
        <h4>Important Note on Normalization</h4>
        The solvers work best with z-score normalized data.
        That means that the columns are normalized to have zero mean and a standard deviation of one.
        This can be achieved by using a normalizer node before learning.
        If you have very sparse data (lots of zero values), this normalization will destroy the sparsity.
        In this case it is recommended to only normalize the dense features to exploit the sparsity during
        the calculations (SAG solver with lazy calculation).
            
            <br />
            
        <h4>Solvers</h4>
        The solver is the most important choice you make as it will dictate which algorithm is used to solve the problem.
        <ul>
        <li>
        	<b>Iteratively reweighted least squares</b> This solver uses an iterative optimization approach which is also
        	sometimes termed Fisher's scoring, to calculate the model. It works well for small tables with only view columns
        	but fails on larger tables. Note that it is the most error prone solver because it can't calculate a model if the
        	data is linearly separable (see Potential Errors and Error Handling for more information).
        	This solver is also not capable of dealing with tables where there are more columns than rows because it does not
        	support regularization.
        </li>
        <li>
        	<b>Stochastic average gradient (SAG)</b> This solver implements a variant of stochastic gradient descent which tends to
        	converge considerably faster than vanilla stochastic gradient descent. For more information on the algorithm see 
        	the following <a href="https://arxiv.org/abs/1309.2388">paper</a>. It works well for large tables and also tables with
        	more columns than rows. Note that in the later case a regularization prior other than <i>uniform</i> must be selected.
        	The default learning rate of 0.01 was selected because it often works well but ultimately the optimal learning rate always
        	depends on the data and should be treated as a hyperparameter.
        </li>
        <li>
        	<b>Stochastic gradient descent (SGD)</b> This solver implements vanilla stochastic gradient descent. Similar to the stochastic
        	average gradient algorithm above, it can deal with large tables of varying shape but it tends to be much slower.
        	It also allows to add regularization but usually requires a much smaller learning rate than the SAG algorithm to perform well.
        </li>
        </ul>

        <h4>Learning Rate/Step Size Strategy</h4>
        Only relevant for the SAG and SGD solvers.
        The learning rate strategy provides the learning rates for the gradient descent.
        When selecting a learning rate strategy and initial learning rate keep in mind that there is always a trade off
        between the size of the learning rate and the number of epochs that are required to converge to a solution.
        With a smaller learning rate the solver will take longer to find a solution but if the learning rate is too large
        it might skip over the optimal solution and diverge in the worst case.
        <ul>
        <li>
        	<b>Fixed</b> The initial learning rate is used for the complete training. Works well for the SAG solver but SGD is
        	not guaranteed to converge with this strategy. Note that the SAG solver generally works well with bigger learning
        	rates while the SGD solver requires small learning rates in order to perform well.
        </li>
        <li>
        	<b>Annealing</b> Reduces the learning rate after each epoch. The mathematical form is
        	a = a_0 / (1 + k/t) where a_0 is the initial learning rate, k is the decay rate and t is the current epoch.
        	This strategy is currently only supported by the SGD solver.
		</li>        
		<li>
			<b>Line Search</b> This experimental strategy performs a line search for the Lipschitz constant and uses the
			estimate to calculate the optimal learning rate. It is only supported by the SAG solver.
		</li>
        </ul>
        
        <h4>Regularization</h4>
        The SAG and SGD solvers optimize the problem using <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">
        maximum a posteriori estimation</a> which allows to specify a prior distribution for the coefficients of the resulting model.
        This form of regularization is the Bayesian version of other regularization approaches such as Ridge or LASSO.
        Currently the following priors are supported:
        <ul>
        <li>
        	<b>Uniform</b> This prior corresponds to no regularization at all and is the default. It essentially means that all values
        	are equally likely for the coefficients.
        </li>
        <li>
        	<b>Gauss</b> The coefficients are assumed to be normally distributed. This prior keeps the coefficients from becoming
        	too large but does not force them to be zero. Using this prior is equivalent to using ridge regression (L2) with
        	a lambda of 1/prior_variance. 
        </li>
        <li>
        	<b>Laplace</b> The coefficients are assumed to follow a Laplace or double exponential distribution. It tends to produce
        	sparse solutions by forcing unimportant coefficients to be zero. It is therefore related to the LASSO (also known as
        	L1 regularization).
        </li>
        </ul>
        
            
        <h4>Potential Errors and Error Handling</h4>
        The computation of the model is an iterative optimization process that requires some properties of the data set.
        This requires a reasonable distribution of the target values and non-constant, uncorrelated columns. While
        some of these properties are checked during the node execution you may still run into errors during the 
        computation. The list below gives some ideas what might go wrong and how to avoid such situations.
        <ul>
        <li>
            <b>Insufficient Information</b> This is the case when the data does not provide enough information about
            one or more target categories. Try to get more data or remove rows for target categories that may cause 
            the error. If you are interested in a model for one target category make sure to group the target
            column before. For instance, if your data contains as target categories the values "A", "B", ..., "Z" but 
            you are only interested in getting a model for class "A" you can use a rule engine node to convert your
            target into "A" and "not A".
        </li>
        <li>
            <b>Violation of Independence</b> Logistic Regression is based on the assumption of statistical independence.
            A common preprocessing step is to us a correlation filter to remove highly correlated learning columns. 
            Use a "Linear Correlation" along with a "Correlation Filter" node to remove redundant columns, whereby often
            it's sufficient to compute the correlation model on a subset of the data only.
        </li>
        <li>
            <b>Separation</b> Please see <a href="http://en.wikipedia.org/wiki/Separation_(statistics)"> this article 
            about separation</a> for more information.
        </li>
        </ul>
		</intro>
		<tab name="Settings">
        <option name="Target">
            Select the target column. Only columns with nominal data are allowed. The reference category is empty
            if the domain of the target column is not available. In this case the node determines the domain values right 
            before computing the logistic regression model and chooses the last domain value as the targets reference 
            category.<br />
            By default the target domain values are sorted lexicographically in the output, but you can enforce the 
            order of the target column domain to be preserved by checking the box.
            <br />
            Note, if a target reference column is selected in the dropdown, the checkbox will have no influence on the
            coefficients of the model except that the output representation (e.g. order of rows in the coefficient table)
            may vary.
        </option>
        <option name="Values">
            Specify the independent columns that should be included in the regression model. 
            Numeric and nominal data can be included.<br />
            By default the domain values (categories) of nominal valued columns are sorted lexicographically, 
            but you can check that the order from the column domain is used. Please note that the first
            category is used as a reference when creating the 
            <a href="http://en.wikipedia.org/wiki/Categorical_variable#Categorical_variables_in_regression">
            dummy variables</a>.
        </option>
        </tab>
        <tab name="Advanced">
        <option name="Number of epochs">
        	Here you can specify the number of learning epochs you want to perform. That is the number of times you want
        	to iterate over the full table. This value determines to a large extend how long learning will take. A too small
        	number can result into a bad model and the learning is stopped early if convergence is detected therefore it is recommended
        	to use a relatively large value.
        </option>
        <option name="Epsilon">
        	This value is used to determine whether the model converged. If the relative change of all coefficients is smaller than epsilon,
        	the training is stopped.
        </option>
        <option name="Perform calculations lazily">
        	If selected, the optimization is performed lazily i.e. the coefficients are only updated
        	if their corresponding feature is actually present in the current sample. Usually faster than the normal version especially for sparse
        	data (that is data where for the most rows the most values are zero). Currently only supported by the SAG solver.
        </option>
        <option name="Learning rate strategy">
        	The strategy provides the learning rates for the optimization process. Only important for the SAG and SGD solvers. For more information
        	see the paragraph on learning rate strategies above.
        </option>
        <option name="Initial learning rate">
        	The learning rate to start with. Note that the default of 0.01 is selected to work well with the SAG solver.
        	The SGD solver usually requires a learning rate that is an order of magnitude smaller (if you use a fixed learning
        	rate strategy).
        </option>
        <option name="Decay rate">
        	The decay rate used by the annealing learning rate strategy.
        </option>
        <option name="Prior">
        	The prior distribution for the coefficients. See the paragraph on regularization above for more details.
        </option>
        <option name="Variance">
        	The variance of the prior distribution. A larger variance corresponds to a less regularization.
        </option>
        <option name="Hold data in memory">
        	If selected, the data is read into an internal data structure which results into a tremendous speed up.
        	It is highly recommended to use this option if you have enough main memory available especially if you use the SAG or SGD solver.
        </option>
        <option name="Use seed">
        	Check if you want to use a static seed. Recommended for reproducible results if you use the SAG or SGD solver.
        </option>
        <option name="Seed">
        	The static seed to use. A click on the "New" button generates a new seed.
        </option>
        </tab>
        	
	</fullDescription>
    <ports>
        <inPort index="0" name="Input data">Table on which to perform regression. The input must not contain missing values, you have to fix them by e.g. using the Missing Values node.</inPort>
        <outPort index="0" name="Model for Predictor">Model to connect to a predictor node.</outPort>
        <outPort index="1" name="Coefficients and Statistics">Coefficients and statistics of the logistic regression model.</outPort>
	</ports>
	<!-- 
	<views>
		<view index="0" name="Logistic Regression Result View">
    		Displays the estimated coefficients and error statistics. Note, 
    		that the estimated coefficients are not reliable when the standard
    		error is high. If you are using regularization, the error statistics will be
    		biased but the magnitude of the coefficients will usually be a good indicator
    		of their importance.
		</view>
	</views>
		 -->
</knimeNode>
